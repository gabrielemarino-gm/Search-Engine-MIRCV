TODO LIST

1. Controllare Preprocesser senza -ss
2. File per il DocumentIndex
3. Iniziare DAAT e PENSARE strutture per nextGEQ


OPZIONALI
4. Compressione (a scelta)
5. In input dovrebbe essere preso il file tar.gz, anziche il tsv

# -----------------------------------------------------------------------------------------------
# -----------------------------------------------------------------------------------------------
# -----------------------------------------------------------------------------------------------

# SPIMI-INVERT(token_stream) Token = É una coppia Term-DocID

#     output file = NEWFILE()
#     dictionary = NEWHASH()
#     while (free memory available)
#     do token <— next(token_stream)
#         if term(token) NOT IN dictionary
#             then postings_list = ADDToDICTIONARY (dictionary, term(token))
#         else postings_list = GETPosTIngsLIsT(dictionary, term(token))
#         if full (postings_list)
#             then postings_list = DoUbLEPostINgsLIsT (dictionary, term(token))
#         ADDToPosTINgsLIsT (postings list, doclD(token))
#     sorted terms <— SORTTerMs(dictionary)
#     WRITE-BLOCKToDIsk(sorted_terms, dictionary, output file)
#     return output_file


# ----------------GLI APPUNTI DI MATTE----------------
# Supponiamo che preprocessare la query ritorni N termini
# Vogliamo ora calcolare TFIDF tra query e documenti D contenenti almeno uno o tutti gli N termini
# TFIDF = TFd,t * IDFt

# TFd,t = Quante volte il termine t appare nel documento d
# IDFt = log( Numero totale di documenti / Numero di documenti dove t appare almeno una volta)

# Per calcolare TFd,t ci serve:
# 1- Quante volte t appare in d (gia presente nel Posting di quel documento per quel termine)

# Per calcolare IDFt ci serve:
# 1- Totale documenti nel Corpus (gia calcolato in fase di creazione dell inverted index)
# 2- In quanti documenti t appare almeno una volta (ossia il numero di posting che ci sono per quel termine)

# IDFt si calcola immediatamente per ogni termine guardando il vocabolario
# TFf,t bisogna scorrere le posting list del termine

# STATISTICA BF25 VA ANCORA FATTA
#   Per questo bisogna calcolare average_document_lenght (calcolato in fase di crezione dell'indice)
#    e le lunghezze (in token) di ogni documento (anche questo calcolabile in fase di crezione dell'indice)

# NB: La maggior parte dell'overhead di tutte queste cose e' dato dal dover leggere dal disco, in quanto
#      si deve assumere che tutte queste strutture non entrano in memoria tutte insieme

# NEXTGEQ VA ANCORA FATTO




# BLOCCHI DI SKIP
# Ogni TermInfo deve anche contenere un Offset dove trovare il primo BlockSkip e idealmente quanti altri
#   ce ne sono. Ogni blocco dovrebbe contenere pezzetti di PostingList lunghi sqrt(posting_totali)
#   Ogni blocco dovrebbe essere compresso a se stante, perlomeno le frequenze che sono Unarie
#   Ogni blocco contiene:
#       - maxDocID dentro quel blocco
#       - docID e freq offsets
#       - Posting contenuti
